<!DOCTYPE HTML>
<!--
	Bodruzzamn Khan
-->
<html>
	<head>
		<title>Machine Learning-Bodruzzaman Khan</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Bodruzzaman Khan</strong> Portfolio</a>
									<ul class="icons">

										<li><a href="http://www.linkedin.com/in/bodruzzaman-khan" class="icon brands fa-linkedin" target="_blank" style="color: #1DA1F2;"><span class="label" >Linkedin</span></a></li>

										<li>
											  <a href="https://www.researchgate.net/profile/Bodruzzaman-Khan-2" target="_blank" class="icon brands fa-researchgate ">
											    <span class="label" >ResearchGate</span>
											  </a>
										</li>

										<!-- <li>
											  <a href="https://www.researchgate.net/profile/Bodruzzaman-Khan-2" target="_blank" class="icon">
										        <img src="./icon/rg.png" alt="ResearchGate" style="width: 1.8em; height: 1.8em; vertical-align: top;"> 
										    </a>
										</li> -->

										<li>
										    <a href="https://scholar.google.com/citations?user=leS0sXoAAAAJ&hl=en" target="_blank" class="icon brands fa-scholar">
										        <img src="./icon/gscholar.png" alt="Google Scholar" style="width: 2.1em; height: 2.1em; vertical-align: text-bottom;"> 
										    </a>
										</li>

										<li><a href="https://github.com/Bodruzzaman-Khan" class="icon brands fa-github" target="_blank" style="color: #181717;"><span class="label">Github</span></a></li>

										
										<li><a href="https://orcid.org/0009-0002-7430-4376" class="icon brands fa-orcid" target="_blank" style="color: green;"><span class="label">Orcid</span></a></li>

										<!-- <li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li> -->
										<!-- <li><a href="#" class="icon brands fa-snapchat-ghost"><span class="label">Snapchat</span></a></li> -->
										<!-- <li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li> -->
										<li>
										    <a href="mailto:godruzzamankhan.sau@gmail.com" target="_blank" class="icon fas fa-envelope" style="color: grren;  border-radius: 0px; padding: 8px;">
										        <span class="label">Email</span>
										    </a>
										</li>


									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Machine Learning Projects</h1>
									</header>

									<!-- Content -->
										<!-- <h2 id="content">Sample Content</h2>
										<p>Praesent ac adipiscing ullamcorper semper ut amet ac risus. Lorem sapien ut odio odio nunc. Ac adipiscing nibh porttitor erat risus justo adipiscing adipiscing amet placerat accumsan. Vis. Faucibus odio magna tempus adipiscing a non. In mi primis arcu ut non accumsan vivamus ac blandit adipiscing adipiscing arcu metus praesent turpis eu ac lacinia nunc ac commodo gravida adipiscing eget accumsan ac nunc adipiscing adipiscing lorem ipsum dolor sit amet nullam veroeros adipiscing.</p>
								 -->

									<hr class="major" />
										<div class="row gtr-200">
													<!-- <h4>Left &amp; Right</h4> -->
													
													<h2 id="content"><span class="image right"><img src="images/pic01.jpg" alt="" /></span><a href="machine_learning/project_0.html" >Scientific Machine Learning in Energy Systems </a>
													<div>
														     <i>SciML, Physics-Inspired Deep Learning, Power System</i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_0.html" style="color: #b22222;"> <i>See more</i> </a>
        											
    												</div></h2>

											<!-- </div> -->
										</div>

									<hr class="major" />
										<div class="row gtr-200">
											<!-- <div class="col-6 col-12-medium"> -->
												<!-- Image -->
													<!-- <h4>Left &amp; Right</h4> -->
													<h2 id="content">
														<span class="image left"><img src="images/bayesian_.png" alt="" /></span> <a href="machine_learning/project_1.html">Bayesian optimization in machine learning and deep learning</a> 
														<div>
														     <i>Tree-structured Parzen Estimator, Gaussian Process, Hyperparameter Optimization</i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_1.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>
    												</h2>


										</div>
										<hr class="major" />
													<div class="row gtr-200">
													<!-- <h4>Left &amp; Right</h4> -->
													
													<h2 id="content"><span class="video right"> <!-- Use video class here -->
												            <video autoplay muted loop>
												                <source src="videos/dot-product-distribution-bzk.mp4" type="video/mp4">
												                Your browser does not support the video tag.
												            </video>
												        </span>
														<a href="machine_learning/project_2.html">Dimensionality's Effect on Variance in Scaled Attention 

														</a> <!-- class="color-project-title" -->
														
														<div>
														     <i>Dimensionality, Variance, Scaled Dot Product, Attention, Transformer</i><br><br>
														     While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem. <a href="machine_learning/project_2.html" style="color: #b22222;"> <i>See more</i> </a>

    												</div></h2>


											<!-- </div> -->
										</div>
										
										<hr class="major" />
										<div class="row gtr-200">
											<!-- <div class="col-6 col-12-medium"> -->
												<!-- Image -->
													<!-- <h4>Left &amp; Right</h4> -->
													<h2 id="content">
														<span class="image left"><img src="images/mldl_.png" alt="" /></span> <a href="machine_learning/project_3.html">ML-DL: A Fusion Learning Approach</a> 
														<div>
														     <i>Hybrid Learning, Convolutional Neural Network, Random Forest (RF), XGBoost, GaussianNB (GNB), Support Vector Machines (SVM), Multinomial Logistic Regression (MLR), K-Nearest Neighbor (KNN) </i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_3.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>
    												</h2>


										</div>

										<hr class="major" />
										<div class="row gtr-200">
											<!-- <div class="col-6 col-12-medium"> -->
												<!-- Image -->
													<!-- <h4>Left &amp; Right</h4> -->
													<h2 id="content">
														<span class="image right"><img src="images/xai_.png" alt="" /></span> <a href="machine_learning/project_4.html">Interpretable AI for Land Cover Classification</a> 
														<div>
														     <i> Environment, AI, Regression, Interpretation</i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_4.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>
    												</h2>


										</div>


										<hr class="major" />
										<div class="row gtr-200">
											<!-- <div class="col-6 col-12-medium"> -->
												<!-- Image -->
													<!-- <h4>Left &amp; Right</h4> -->
													<h2 id="content"><span class="video left"> <!-- Use video class here -->
												            <video autoplay muted loop>
												                <source src="videos/bzk_fluid_simu_resize.mp4" type="video/mp4">
												                Your browser does not support the video tag.
												            </video>
												        </span>
														<a href="machine_learning/project_5.html">Simulation of Fluid Dynamics Using Physics-Inspired Deep Learning

														</a> <!-- class="color-project-title" -->
														
														<div>
														     <i>SciML, PINNs, PDE, Differentiable Simulation, Fluid Dynamics</i><br><br>
														     Employing physics-inspired deep learning mechanisms I attempted to simulate fluid dynamics around a rotating object. Such an DL approach aids in solving complex mathematical functions.
															<a href="machine_learning/project_5.html" style="color: #b22222;"> <i>See more</i> </a>
														    
    												   </div>
    												</h2>
										</div>



										<div class="row gtr-200">
											<!-- <div class="col-6 col-12-medium"> -->
												<!-- Image -->
													<!-- <h4>Left &amp; Right</h4> -->
													<h2 id="content">
														<span class="image right"><img src="images/pic01.jpg" alt="" /></span> <a href="machine_learning/project_6.html">Pixel-based LULC Classification Using Machine Learning</a> 
														<div>
														     <i>Land Use, Land Cover, ML, Image Analysis, Supervised Classification </i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_6.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>
    												</h2>


										</div>


										<hr class="major" />

										<div class="row gtr-200">
											<!-- <div class="col-6 col-12-medium"> -->
												<!-- Image -->
													<!-- <h4>Left &amp; Right</h4> -->
													<h2 id="content"><span class="image left"><img src="images/pic01.jpg" alt="" /></span><a href="machine_learning/project_7.html" >Land Cover Prediction Using Custom ML Models </a>
														<div>
														     <i>Land Use, Land Cover, ML, Image Analysis, Supervised Classification </i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_7.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>
    												</h2>


										</div>
										<hr class="major" />
										<div class="row gtr-200">
													<!-- <h4>Left &amp; Right</h4> -->
													
													<h2 id="content"><span class="image right"><img src="images/pic01.jpg" alt="" /></span> <a href="machine_learning/project_8.html">Developing Novel ViT Models for Real-Life Object Identification </a>
														<div>
														     <i>Transformer, Vision Transformer, Attention Mechanism, Supervised Learning </i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_8.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>

													</h2>

											<!-- </div> -->
										</div>
										<hr class="major" />

										<div class="row gtr-200">
											<!-- <div class="col-6 col-12-medium"> -->
												<!-- Image -->
													<!-- <h4>Left &amp; Right</h4> -->
													<h2 id="content"><span class="image left"><img src="images/pic01.jpg" alt="" /></span> <a href="machine_learning/project_9.html"> DL-based Spatio-temporal Hybrid Model for Outage Prediction</a> 
														<div>
														     <i>Electricity, Hybrid Learning, Deep Learning </i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_9.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>

													</h2>


										</div>
										<hr class="major" />
										<div class="row gtr-200">
													<!-- <h4>Left &amp; Right</h4> -->
													
													<h2 id="content"><span class="image right"><img src="images/ccnn.jpg" alt="" /></span><a href="machine_learning/project_10.html" >Tomato Leaf Disease Classification Using Custom CNN </a>
														<div>
														     <i>Convolutional Neural Network, Artificial Neural Network, Deep Learning </i><br><br>
														     
														     <!-- While exploring the minute component of attention, one question that triggered me is why  
														    <strong><a href="https://arxiv.org/abs/1706.03762" target="_blank"> Researchers</a></strong> incorporated scaling operations. 
														    I observed that as the embedding dimensions increase, the variances also rise, leading to uncontrolled variability, potentially resulting in the vanishing gradient problem.  -->
														    <a href="machine_learning/project_10.html" style="color: #b22222;"> <i>See more</i> </a>

    													</div>
    											</h2>

											<!-- </div> -->
										</div>
										

								</section>

						<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Copyright 2024 Bodruzzaman Khan. All rights reserved. Acknowledgments: <a href="https://unsplash.com">Unsplash</a>, <a href="https://html5up.net">HTML5 UP</a>. Last updated: <span id="last-update"></span>.</p>
								</footer>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<!-- <section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section> -->

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">About Me</a></li>
										<li><a href="resume.html">Resume</a></li>
										<li><a href="publications.html">Publications</a></li>
										<li><a href="research.html">Research</a></li>
										<li><a href="academic.html">Academic</a></li>
										<li><a href="skills.html">Skills & Interests</a></li>
										<li><a href="experience.html">Experience</a></li>
										<li>
											<span class="opener">Projects</span>
											<ul>
												<li><a href="machine_learning.html">Machine Learning</a></li>
												<li><a href="remotesensing.html">GIS & Remote Sensing</a></li>
												<li><a href="python.html">Python</a></li>
												<li><a href="cad.html">CAD</a></li>
												<li><a href="otherproject.html">Others</a></li>
											</ul>
										</li>
										<li><a href="achievement.html">Awards & Certifications</a></li>
										<li><a href="extracurricular.html">Extracurriculars</a></li>
										
										
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Contact Me</h2>
									</header>
									<p> </p>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="mailto:bodruzzamankhan.sau@gmail.com" target="_blank">bodruzzamankhan.sau<br>@gmail.com</a></li>
										<li class="icon solid fa-phone">
										  <a href="https://wa.me/8801794738300" target="_blank" style="color: inherit;">+880 1794738300</a>
										</li>
										<li class="icon solid fa-home">Sylhet, 
										Bangladesh</li>
									</ul>
								</section>

							<!-- Footer -->
								<!-- <footer id="footer">
									<p class="copyright">&copy; Copyright 2024 Bodruzzaman Khan. All rights reserved. Images: <a href="https://unsplash.com">Unsplash</a>. Powered by: <a href="https://html5up.net">HTML5 UP</a>. Last updated: <span id="last-update"></span>.</p>
								</footer> -->

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>